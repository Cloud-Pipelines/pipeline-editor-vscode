name: XGBoost pipeline
metadata:
  annotations:
    sdk: https://cloud-pipelines.net/pipeline-editor/
implementation:
  graph:
    tasks:
      dataset:
        componentRef:
          url: https://raw.githubusercontent.com/Ark-kun/pipelines/60a2612541ec08c6a85c237d2ec7525b12543a43/components/datasets/Chicago_Taxi_Trips/component.yaml
          spec:
            name: Chicago Taxi Trips dataset
            description: |
              City of Chicago Taxi Trips dataset: https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew

              The input parameters configure the SQL query to the database.
              The dataset is pretty big, so limit the number of results using the `Limit` or `Where` parameters.
              Read [Socrata dev](https://dev.socrata.com/docs/queries/) for the advanced query syntax
            metadata:
              annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
            inputs:
              - name: Where
                type: String
                default: trip_start_timestamp>="1900-01-01" AND trip_start_timestamp<"2100-01-01"
              - name: Limit
                type: Integer
                default: '1000'
                description: Number of rows to return. The rows are randomly sampled.
              - name: Select
                type: String
                default: trip_id,taxi_id,trip_start_timestamp,trip_end_timestamp,trip_seconds,trip_miles,pickup_census_tract,dropoff_census_tract,pickup_community_area,dropoff_community_area,fare,tips,tolls,extras,trip_total,payment_type,company,pickup_centroid_latitude,pickup_centroid_longitude,pickup_centroid_location,dropoff_centroid_latitude,dropoff_centroid_longitude,dropoff_centroid_location
              - name: Format
                type: String
                default: csv
                description: Output data format. Suports csv,tsv,cml,rdf,json
            outputs:
              - name: Table
                description: Result type depends on format. CSV and TSV have header.
            implementation:
              container:
                image: byrnedo/alpine-curl@sha256:548379d0a4a0c08b9e55d9d87a592b7d35d9ab3037f4936f5ccd09d0b625a342
                command:
                  - sh
                  - '-c'
                  - |
                    set -e -x -o pipefail
                    output_path="$0"
                    select="$1"
                    where="$2"
                    limit="$3"
                    format="$4"
                    mkdir -p "$(dirname "$output_path")"
                    curl --get 'https://data.cityofchicago.org/resource/wrvz-psew.'"${format}" \
                        --data-urlencode '$limit='"${limit}" \
                        --data-urlencode '$where='"${where}" \
                        --data-urlencode '$select='"${select}" \
                        | tr -d '"' > "$output_path"  # Removing unneeded quotes around all numbers
                  - outputPath: Table
                  - inputValue: Select
                  - inputValue: Where
                  - inputValue: Limit
                  - inputValue: Format
        annotations:
          editor.position: '{"x":60,"y":100,"width":180,"height":40}'
        arguments:
          Select: tips,trip_seconds,trip_miles,pickup_community_area,dropoff_community_area,fare,tolls,extras,trip_total
          Where: trip_start_timestamp >= "2019-01-01" AND trip_start_timestamp < "2019-02-01"
      Xgboost 5 fold cross validation for regression:
        componentRef:
          spec:
            name: Xgboost 5 fold cross validation for regression
            inputs:
              - name: data
                type: CSV
              - name: label_column
                type: Integer
                default: '0'
                optional: true
              - name: objective
                type: String
                default: reg:squarederror
                optional: true
              - name: num_iterations
                type: Integer
                default: '200'
                optional: true
            outputs:
              - name: mean_absolute_error
                type: Float
              - name: mean_squared_error
                type: Float
              - name: root_mean_squared_error
                type: Float
              - name: metrics
                type: JsonObject
            metadata:
              annotations:
                author: Alexey Volkov <alexey.volkov@ark-kun.com>
                canonical_location: https://raw.githubusercontent.com/Ark-kun/pipeline_components/master/components/XGBoost/Cross_validation_for_regression/from_CSV/component.yaml
            implementation:
              graph:
                tasks:
                  Split table into folds:
                    componentRef:
                      digest: 9956223bcecc7294ca1afac39b60ada4a935a571d817c3dfbf2ea4a211afe3d1
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/e9b4b29b22a5120daf95b581b0392cd461a906f0/components/dataset_manipulation/split_data_into_folds/in_CSV/component.yaml
                      spec:
                        name: Split table into folds
                        description: |-
                          Splits the data table into the specified number of folds.

                              The data is split into the specified number of folds k (default: 5).
                              Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
                              Each training subsample has (k-1)/k fraction of samples.
                              The train_i subsample is produced by excluding test_i subsample form all samples.

                              Inputs:
                                  table: The data to split by rows
                                  number_of_folds: Number of folds to split data into
                                  random_seed: Random seed for reproducible splitting

                              Outputs:
                                  train_i: The i-th training subsample
                                  test_i: The i-th testing subsample

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        metadata:
                          annotations:
                            author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                            type: CSV
                          - name: number_of_folds
                            type: Integer
                            default: '5'
                            optional: true
                          - name: random_seed
                            type: Integer
                            default: '0'
                            optional: true
                        outputs:
                          - name: train_1
                            type: CSV
                          - name: train_2
                            type: CSV
                          - name: train_3
                            type: CSV
                          - name: train_4
                            type: CSV
                          - name: train_5
                            type: CSV
                          - name: test_1
                            type: CSV
                          - name: test_2
                            type: CSV
                          - name: test_3
                            type: CSV
                          - name: test_4
                            type: CSV
                          - name: test_5
                            type: CSV
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'scikit-learn==0.23.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def split_table_into_folds(
                                    table_path,

                                    train_1_path,
                                    train_2_path,
                                    train_3_path,
                                    train_4_path,
                                    train_5_path,

                                    test_1_path,
                                    test_2_path,
                                    test_3_path,
                                    test_4_path,
                                    test_5_path,

                                    number_of_folds = 5,
                                    random_seed = 0,
                                ):
                                    """Splits the data table into the specified number of folds.

                                    The data is split into the specified number of folds k (default: 5).
                                    Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.
                                    Each training subsample has (k-1)/k fraction of samples.
                                    The train_i subsample is produced by excluding test_i subsample form all samples.

                                    Inputs:
                                        table: The data to split by rows
                                        number_of_folds: Number of folds to split data into
                                        random_seed: Random seed for reproducible splitting

                                    Outputs:
                                        train_i: The i-th training subsample
                                        test_i: The i-th testing subsample

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>

                                    """
                                    import pandas
                                    from sklearn import model_selection

                                    max_number_of_folds = 5

                                    if number_of_folds < 1 or number_of_folds > max_number_of_folds:
                                        raise ValueError('Number of folds must be between 1 and {}.'.format(max_number_of_folds))

                                    df = pandas.read_csv(
                                        table_path,
                                    )
                                    splitter = model_selection.KFold(
                                        n_splits=number_of_folds,
                                        shuffle=True,
                                        random_state=random_seed,
                                    )
                                    folds = list(splitter.split(df))

                                    fold_paths = [
                                        (train_1_path, test_1_path),
                                        (train_2_path, test_2_path),
                                        (train_3_path, test_3_path),
                                        (train_4_path, test_4_path),
                                        (train_5_path, test_5_path),
                                    ]

                                    for i in range(max_number_of_folds):
                                        (train_path, test_path) = fold_paths[i]
                                        if i < len(folds):
                                            (train_indices, test_indices) = folds[i]
                                            train_fold = df.iloc[train_indices]
                                            test_fold = df.iloc[test_indices]
                                        else:
                                            train_fold = df.iloc[0:0]
                                            test_fold = df.iloc[0:0]
                                        train_fold.to_csv(train_path, index=False)
                                        test_fold.to_csv(test_path, index=False)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Split table into folds', description='Splits the data table into the specified number of folds.\n\n    The data is split into the specified number of folds k (default: 5).\n    Each testing subsample has 1/k fraction of samples. The testing subsamples do not overlap.\n    Each training subsample has (k-1)/k fraction of samples.\n    The train_i subsample is produced by excluding test_i subsample form all samples.\n\n    Inputs:\n        table: The data to split by rows\n        number_of_folds: Number of folds to split data into\n        random_seed: Random seed for reproducible splitting\n\n    Outputs:\n        train_i: The i-th training subsample\n        test_i: The i-th testing subsample\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--number-of-folds", dest="number_of_folds", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--random-seed", dest="random_seed", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--train-1", dest="train_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--train-2", dest="train_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--train-3", dest="train_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--train-4", dest="train_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--train-5", dest="train_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--test-1", dest="test_1_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--test-2", dest="test_2_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--test-3", dest="test_3_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--test-4", dest="test_4_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--test-5", dest="test_5_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = split_table_into_folds(**_parsed_args)
                            args:
                              - '--table'
                              - inputPath: table
                              - if:
                                  cond:
                                    isPresent: number_of_folds
                                  then:
                                    - '--number-of-folds'
                                    - inputValue: number_of_folds
                              - if:
                                  cond:
                                    isPresent: random_seed
                                  then:
                                    - '--random-seed'
                                    - inputValue: random_seed
                              - '--train-1'
                              - outputPath: train_1
                              - '--train-2'
                              - outputPath: train_2
                              - '--train-3'
                              - outputPath: train_3
                              - '--train-4'
                              - outputPath: train_4
                              - '--train-5'
                              - outputPath: train_5
                              - '--test-1'
                              - outputPath: test_1
                              - '--test-2'
                              - outputPath: test_2
                              - '--test-3'
                              - outputPath: test_3
                              - '--test-4'
                              - outputPath: test_4
                              - '--test-5'
                              - outputPath: test_5
                    arguments:
                      table:
                        graphInput:
                          inputName: data
                  Xgboost train:
                    componentRef:
                      digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml
                      spec:
                        name: Xgboost train
                        description: |-
                          Train an XGBoost model.

                              Args:
                                  training_data_path: Path for the training data in CSV format.
                                  model_path: Output path for the trained model in binary XGBoost format.
                                  model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                  starting_model_path: Path for the existing trained model to start from.
                                  label_column: Column containing the label data.
                                  num_boost_rounds: Number of boosting iterations.
                                  booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                  objective: The learning task and the corresponding learning objective.
                                      See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                      The most common values are:
                                      "reg:squarederror" - Regression with squared loss (default).
                                      "reg:logistic" - Logistic regression.
                                      "binary:logistic" - Logistic regression for binary classification, output probability.
                                      "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                      "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                      "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: training_data
                            type: CSV
                          - name: starting_model
                            type: XGBoostModel
                            optional: true
                          - name: label_column
                            type: Integer
                            default: '0'
                            optional: true
                          - name: num_iterations
                            type: Integer
                            default: '10'
                            optional: true
                          - name: booster_params
                            type: JsonObject
                            optional: true
                          - name: objective
                            type: String
                            default: reg:squarederror
                            optional: true
                          - name: booster
                            type: String
                            default: gbtree
                            optional: true
                          - name: learning_rate
                            type: Float
                            default: '0.3'
                            optional: true
                          - name: min_split_loss
                            type: Float
                            default: '0'
                            optional: true
                          - name: max_depth
                            type: Integer
                            default: '6'
                            optional: true
                        outputs:
                          - name: model
                            type: XGBoostModel
                          - name: model_config
                            type: XGBoostModelConfig
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_train(
                                    training_data_path,  # Also supports LibSVM
                                    model_path,
                                    model_config_path,
                                    starting_model_path = None,

                                    label_column = 0,
                                    num_iterations = 10,
                                    booster_params = None,

                                    # Booster parameters
                                    objective = 'reg:squarederror',
                                    booster = 'gbtree',
                                    learning_rate = 0.3,
                                    min_split_loss = 0,
                                    max_depth = 6,
                                ):
                                    '''Train an XGBoost model.

                                    Args:
                                        training_data_path: Path for the training data in CSV format.
                                        model_path: Output path for the trained model in binary XGBoost format.
                                        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                        starting_model_path: Path for the existing trained model to start from.
                                        label_column: Column containing the label data.
                                        num_boost_rounds: Number of boosting iterations.
                                        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                        objective: The learning task and the corresponding learning objective.
                                            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                            The most common values are:
                                            "reg:squarederror" - Regression with squared loss (default).
                                            "reg:logistic" - Logistic regression.
                                            "binary:logistic" - Logistic regression for binary classification, output probability.
                                            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        training_data_path,
                                    )

                                    training_data = xgboost.DMatrix(
                                        data=df.drop(columns=[df.columns[label_column]]),
                                        label=df[df.columns[label_column]],
                                    )

                                    booster_params = booster_params or {}
                                    booster_params.setdefault('objective', objective)
                                    booster_params.setdefault('booster', booster)
                                    booster_params.setdefault('learning_rate', learning_rate)
                                    booster_params.setdefault('min_split_loss', min_split_loss)
                                    booster_params.setdefault('max_depth', max_depth)

                                    starting_model = None
                                    if starting_model_path:
                                        starting_model = xgboost.Booster(model_file=starting_model_path)

                                    model = xgboost.train(
                                        params=booster_params,
                                        dtrain=training_data,
                                        num_boost_round=num_iterations,
                                        xgb_model=starting_model
                                    )

                                    # Saving the model in binary format
                                    model.save_model(model_path)

                                    model_config_str = model.save_config()
                                    with open(model_config_path, 'w') as model_config_file:
                                        model_config_file.write(model_config_str)

                                import json
                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost train', description='Train an XGBoost model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary XGBoost format.\n        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n        num_boost_rounds: Number of boosting iterations.\n        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n        objective: The learning task and the corresponding learning objective.\n            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n            The most common values are:\n            "reg:squarederror" - Regression with squared loss (default).\n            "reg:logistic" - Logistic regression.\n            "binary:logistic" - Logistic regression for binary classification, output probability.\n            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation\n            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_train(**_parsed_args)
                            args:
                              - '--training-data'
                              - inputPath: training_data
                              - if:
                                  cond:
                                    isPresent: starting_model
                                  then:
                                    - '--starting-model'
                                    - inputPath: starting_model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - if:
                                  cond:
                                    isPresent: num_iterations
                                  then:
                                    - '--num-iterations'
                                    - inputValue: num_iterations
                              - if:
                                  cond:
                                    isPresent: booster_params
                                  then:
                                    - '--booster-params'
                                    - inputValue: booster_params
                              - if:
                                  cond:
                                    isPresent: objective
                                  then:
                                    - '--objective'
                                    - inputValue: objective
                              - if:
                                  cond:
                                    isPresent: booster
                                  then:
                                    - '--booster'
                                    - inputValue: booster
                              - if:
                                  cond:
                                    isPresent: learning_rate
                                  then:
                                    - '--learning-rate'
                                    - inputValue: learning_rate
                              - if:
                                  cond:
                                    isPresent: min_split_loss
                                  then:
                                    - '--min-split-loss'
                                    - inputValue: min_split_loss
                              - if:
                                  cond:
                                    isPresent: max_depth
                                  then:
                                    - '--max-depth'
                                    - inputValue: max_depth
                              - '--model'
                              - outputPath: model
                              - '--model-config'
                              - outputPath: model_config
                    arguments:
                      training_data:
                        taskOutput:
                          outputName: train_1
                          taskId: Split table into folds
                          type: CSV
                      label_column:
                        graphInput:
                          inputName: label_column
                      num_iterations:
                        graphInput:
                          inputName: num_iterations
                      objective:
                        graphInput:
                          inputName: objective
                  Xgboost predict:
                    componentRef:
                      digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml
                      spec:
                        name: Xgboost predict
                        description: |-
                          Make predictions using a trained XGBoost model.

                              Args:
                                  data_path: Path for the feature data in CSV format.
                                  model_path: Path for the trained model in binary XGBoost format.
                                  predictions_path: Output path for the predictions.
                                  label_column: Column containing the label data.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: data
                            type: CSV
                          - name: model
                            type: XGBoostModel
                          - name: label_column
                            type: Integer
                            optional: true
                        outputs:
                          - name: predictions
                            type: Text
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_predict(
                                    data_path,  # Also supports LibSVM
                                    model_path,
                                    predictions_path,
                                    label_column = None,
                                ):
                                    '''Make predictions using a trained XGBoost model.

                                    Args:
                                        data_path: Path for the feature data in CSV format.
                                        model_path: Path for the trained model in binary XGBoost format.
                                        predictions_path: Output path for the predictions.
                                        label_column: Column containing the label data.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    from pathlib import Path

                                    import numpy
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        data_path,
                                    )

                                    if label_column is not None:
                                        df = df.drop(columns=[df.columns[label_column]])

                                    testing_data = xgboost.DMatrix(
                                        data=df,
                                    )

                                    model = xgboost.Booster(model_file=model_path)

                                    predictions = model.predict(testing_data)

                                    Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                                    numpy.savetxt(predictions_path, predictions)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost predict', description='Make predictions using a trained XGBoost model.\n\n    Args:\n        data_path: Path for the feature data in CSV format.\n        model_path: Path for the trained model in binary XGBoost format.\n        predictions_path: Output path for the predictions.\n        label_column: Column containing the label data.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_predict(**_parsed_args)
                            args:
                              - '--data'
                              - inputPath: data
                              - '--model'
                              - inputPath: model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - '--predictions'
                              - outputPath: predictions
                    arguments:
                      data:
                        taskOutput:
                          outputName: test_1
                          taskId: Split table into folds
                          type: CSV
                      model:
                        taskOutput:
                          outputName: model
                          taskId: Xgboost train
                          type: XGBoostModel
                      label_column:
                        graphInput:
                          inputName: label_column
                  Pandas Transform DataFrame in CSV format:
                    componentRef:
                      digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml
                      spec:
                        name: Pandas Transform DataFrame in CSV format
                        description: |-
                          Transform DataFrame loaded from a CSV file.

                              Inputs:
                                  table: Table to transform.
                                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                      The DataFrame variable is called "df".
                                      Examples:
                                      - `df['prod'] = df['X'] * df['Y']`
                                      - `df = df[['X', 'prod']]`
                                      - `df.insert(0, "is_positive", df["X"] > 0)`

                              Outputs:
                                  transformed_table: Transformed table.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                            type: CSV
                          - name: transform_code
                            type: PythonCode
                        outputs:
                          - name: transformed_table
                            type: CSV
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def Pandas_Transform_DataFrame_in_CSV_format(
                                    table_path,
                                    transformed_table_path,
                                    transform_code,
                                ):
                                    '''Transform DataFrame loaded from a CSV file.

                                    Inputs:
                                        table: Table to transform.
                                        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                            The DataFrame variable is called "df".
                                            Examples:
                                            - `df['prod'] = df['X'] * df['Y']`
                                            - `df = df[['X', 'prod']]`
                                            - `df.insert(0, "is_positive", df["X"] > 0)`

                                    Outputs:
                                        transformed_table: Transformed table.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas

                                    df = pandas.read_csv(
                                        table_path,
                                    )
                                    # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                                    namespace = locals()
                                    exec(transform_code, namespace)
                                    namespace['df'].to_csv(
                                        transformed_table_path,
                                        index=False,
                                    )

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
                            args:
                              - '--table'
                              - inputPath: table
                              - '--transform-code'
                              - inputValue: transform_code
                              - '--transformed-table'
                              - outputPath: transformed_table
                    arguments:
                      table:
                        taskOutput:
                          outputName: test_1
                          taskId: Split table into folds
                          type: CSV
                      transform_code: df = df[["tips"]]
                  Remove header:
                    componentRef:
                      digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml
                      spec:
                        name: Remove header
                        description: Remove the header line from CSV and TSV data (unconditionally)
                        metadata:
                          annotations:
                            author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                        outputs:
                          - name: table
                        implementation:
                          container:
                            image: alpine
                            command:
                              - sh
                              - '-exc'
                              - |
                                mkdir -p "$(dirname "$1")"
                                tail -n +2 <"$0" >"$1"
                              - inputPath: table
                              - outputPath: table
                    arguments:
                      table:
                        taskOutput:
                          outputName: transformed_table
                          taskId: Pandas Transform DataFrame in CSV format
                          type: CSV
                  Calculate regression metrics from csv:
                    componentRef:
                      digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml
                      spec:
                        name: Calculate regression metrics from csv
                        description: |-
                          Calculates regression metrics.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: true_values
                          - name: predicted_values
                        outputs:
                          - name: number_of_items
                            type: Integer
                          - name: max_absolute_error
                            type: Float
                          - name: mean_absolute_error
                            type: Float
                          - name: mean_squared_error
                            type: Float
                          - name: root_mean_squared_error
                            type: Float
                          - name: metrics
                            type: JsonObject
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def calculate_regression_metrics_from_csv(
                                    true_values_path,
                                    predicted_values_path,
                                ):
                                    '''Calculates regression metrics.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import math
                                    import numpy

                                    true_values = numpy.loadtxt(true_values_path, dtype=numpy.float64)
                                    predicted_values = numpy.loadtxt(predicted_values_path, dtype=numpy.float64)

                                    if len(predicted_values.shape) != 1:
                                        raise NotImplemented('Only single prediction values are supported.')
                                    if len(true_values.shape) != 1:
                                        raise NotImplemented('Only single true values are supported.')

                                    if predicted_values.shape != true_values.shape:
                                        raise ValueError('Input shapes are different: {} != {}'.format(predicted_values.shape, true_values.shape))

                                    number_of_items = true_values.size
                                    errors = (true_values - predicted_values)
                                    abs_errors = numpy.abs(errors)
                                    squared_errors = errors ** 2
                                    max_absolute_error = numpy.max(abs_errors)
                                    mean_absolute_error = numpy.average(abs_errors)
                                    mean_squared_error = numpy.average(squared_errors)
                                    root_mean_squared_error = math.sqrt(mean_squared_error)
                                    metrics = dict(
                                        number_of_items=number_of_items,
                                        max_absolute_error=max_absolute_error,
                                        mean_absolute_error=mean_absolute_error,
                                        mean_squared_error=mean_squared_error,
                                        root_mean_squared_error=root_mean_squared_error,
                                    )

                                    return (
                                        number_of_items,
                                        max_absolute_error,
                                        mean_absolute_error,
                                        mean_squared_error,
                                        root_mean_squared_error,
                                        metrics,
                                    )

                                def _serialize_json(obj) -> str:
                                    if isinstance(obj, str):
                                        return obj
                                    import json
                                    def default_serializer(obj):
                                        if hasattr(obj, 'to_struct'):
                                            return obj.to_struct()
                                        else:
                                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                                def _serialize_float(float_value: float) -> str:
                                    if isinstance(float_value, str):
                                        return float_value
                                    if not isinstance(float_value, (float, int)):
                                        raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
                                    return str(float_value)

                                def _serialize_int(int_value: int) -> str:
                                    if isinstance(int_value, str):
                                        return int_value
                                    if not isinstance(int_value, int):
                                        raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
                                    return str(int_value)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Calculate regression metrics from csv', description='Calculates regression metrics.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--true-values", dest="true_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--predicted-values", dest="predicted_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=6)
                                _parsed_args = vars(_parser.parse_args())
                                _output_files = _parsed_args.pop("_output_paths", [])

                                _outputs = calculate_regression_metrics_from_csv(**_parsed_args)

                                _output_serializers = [
                                    _serialize_int,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_json,

                                ]

                                import os
                                for idx, output_file in enumerate(_output_files):
                                    try:
                                        os.makedirs(os.path.dirname(output_file))
                                    except OSError:
                                        pass
                                    with open(output_file, 'w') as f:
                                        f.write(_output_serializers[idx](_outputs[idx]))
                            args:
                              - '--true-values'
                              - inputPath: true_values
                              - '--predicted-values'
                              - inputPath: predicted_values
                              - '----output-paths'
                              - outputPath: number_of_items
                              - outputPath: max_absolute_error
                              - outputPath: mean_absolute_error
                              - outputPath: mean_squared_error
                              - outputPath: root_mean_squared_error
                              - outputPath: metrics
                    arguments:
                      true_values:
                        taskOutput:
                          outputName: table
                          taskId: Remove header
                      predicted_values:
                        taskOutput:
                          outputName: predictions
                          taskId: Xgboost predict
                          type: Text
                  Xgboost train 2:
                    componentRef:
                      digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml
                      spec:
                        name: Xgboost train
                        description: |-
                          Train an XGBoost model.

                              Args:
                                  training_data_path: Path for the training data in CSV format.
                                  model_path: Output path for the trained model in binary XGBoost format.
                                  model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                  starting_model_path: Path for the existing trained model to start from.
                                  label_column: Column containing the label data.
                                  num_boost_rounds: Number of boosting iterations.
                                  booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                  objective: The learning task and the corresponding learning objective.
                                      See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                      The most common values are:
                                      "reg:squarederror" - Regression with squared loss (default).
                                      "reg:logistic" - Logistic regression.
                                      "binary:logistic" - Logistic regression for binary classification, output probability.
                                      "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                      "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                      "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: training_data
                            type: CSV
                          - name: starting_model
                            type: XGBoostModel
                            optional: true
                          - name: label_column
                            type: Integer
                            default: '0'
                            optional: true
                          - name: num_iterations
                            type: Integer
                            default: '10'
                            optional: true
                          - name: booster_params
                            type: JsonObject
                            optional: true
                          - name: objective
                            type: String
                            default: reg:squarederror
                            optional: true
                          - name: booster
                            type: String
                            default: gbtree
                            optional: true
                          - name: learning_rate
                            type: Float
                            default: '0.3'
                            optional: true
                          - name: min_split_loss
                            type: Float
                            default: '0'
                            optional: true
                          - name: max_depth
                            type: Integer
                            default: '6'
                            optional: true
                        outputs:
                          - name: model
                            type: XGBoostModel
                          - name: model_config
                            type: XGBoostModelConfig
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_train(
                                    training_data_path,  # Also supports LibSVM
                                    model_path,
                                    model_config_path,
                                    starting_model_path = None,

                                    label_column = 0,
                                    num_iterations = 10,
                                    booster_params = None,

                                    # Booster parameters
                                    objective = 'reg:squarederror',
                                    booster = 'gbtree',
                                    learning_rate = 0.3,
                                    min_split_loss = 0,
                                    max_depth = 6,
                                ):
                                    '''Train an XGBoost model.

                                    Args:
                                        training_data_path: Path for the training data in CSV format.
                                        model_path: Output path for the trained model in binary XGBoost format.
                                        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                        starting_model_path: Path for the existing trained model to start from.
                                        label_column: Column containing the label data.
                                        num_boost_rounds: Number of boosting iterations.
                                        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                        objective: The learning task and the corresponding learning objective.
                                            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                            The most common values are:
                                            "reg:squarederror" - Regression with squared loss (default).
                                            "reg:logistic" - Logistic regression.
                                            "binary:logistic" - Logistic regression for binary classification, output probability.
                                            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        training_data_path,
                                    )

                                    training_data = xgboost.DMatrix(
                                        data=df.drop(columns=[df.columns[label_column]]),
                                        label=df[df.columns[label_column]],
                                    )

                                    booster_params = booster_params or {}
                                    booster_params.setdefault('objective', objective)
                                    booster_params.setdefault('booster', booster)
                                    booster_params.setdefault('learning_rate', learning_rate)
                                    booster_params.setdefault('min_split_loss', min_split_loss)
                                    booster_params.setdefault('max_depth', max_depth)

                                    starting_model = None
                                    if starting_model_path:
                                        starting_model = xgboost.Booster(model_file=starting_model_path)

                                    model = xgboost.train(
                                        params=booster_params,
                                        dtrain=training_data,
                                        num_boost_round=num_iterations,
                                        xgb_model=starting_model
                                    )

                                    # Saving the model in binary format
                                    model.save_model(model_path)

                                    model_config_str = model.save_config()
                                    with open(model_config_path, 'w') as model_config_file:
                                        model_config_file.write(model_config_str)

                                import json
                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost train', description='Train an XGBoost model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary XGBoost format.\n        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n        num_boost_rounds: Number of boosting iterations.\n        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n        objective: The learning task and the corresponding learning objective.\n            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n            The most common values are:\n            "reg:squarederror" - Regression with squared loss (default).\n            "reg:logistic" - Logistic regression.\n            "binary:logistic" - Logistic regression for binary classification, output probability.\n            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation\n            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_train(**_parsed_args)
                            args:
                              - '--training-data'
                              - inputPath: training_data
                              - if:
                                  cond:
                                    isPresent: starting_model
                                  then:
                                    - '--starting-model'
                                    - inputPath: starting_model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - if:
                                  cond:
                                    isPresent: num_iterations
                                  then:
                                    - '--num-iterations'
                                    - inputValue: num_iterations
                              - if:
                                  cond:
                                    isPresent: booster_params
                                  then:
                                    - '--booster-params'
                                    - inputValue: booster_params
                              - if:
                                  cond:
                                    isPresent: objective
                                  then:
                                    - '--objective'
                                    - inputValue: objective
                              - if:
                                  cond:
                                    isPresent: booster
                                  then:
                                    - '--booster'
                                    - inputValue: booster
                              - if:
                                  cond:
                                    isPresent: learning_rate
                                  then:
                                    - '--learning-rate'
                                    - inputValue: learning_rate
                              - if:
                                  cond:
                                    isPresent: min_split_loss
                                  then:
                                    - '--min-split-loss'
                                    - inputValue: min_split_loss
                              - if:
                                  cond:
                                    isPresent: max_depth
                                  then:
                                    - '--max-depth'
                                    - inputValue: max_depth
                              - '--model'
                              - outputPath: model
                              - '--model-config'
                              - outputPath: model_config
                    arguments:
                      training_data:
                        taskOutput:
                          outputName: train_2
                          taskId: Split table into folds
                          type: CSV
                      label_column:
                        graphInput:
                          inputName: label_column
                      num_iterations:
                        graphInput:
                          inputName: num_iterations
                      objective:
                        graphInput:
                          inputName: objective
                  Xgboost predict 2:
                    componentRef:
                      digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml
                      spec:
                        name: Xgboost predict
                        description: |-
                          Make predictions using a trained XGBoost model.

                              Args:
                                  data_path: Path for the feature data in CSV format.
                                  model_path: Path for the trained model in binary XGBoost format.
                                  predictions_path: Output path for the predictions.
                                  label_column: Column containing the label data.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: data
                            type: CSV
                          - name: model
                            type: XGBoostModel
                          - name: label_column
                            type: Integer
                            optional: true
                        outputs:
                          - name: predictions
                            type: Text
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_predict(
                                    data_path,  # Also supports LibSVM
                                    model_path,
                                    predictions_path,
                                    label_column = None,
                                ):
                                    '''Make predictions using a trained XGBoost model.

                                    Args:
                                        data_path: Path for the feature data in CSV format.
                                        model_path: Path for the trained model in binary XGBoost format.
                                        predictions_path: Output path for the predictions.
                                        label_column: Column containing the label data.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    from pathlib import Path

                                    import numpy
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        data_path,
                                    )

                                    if label_column is not None:
                                        df = df.drop(columns=[df.columns[label_column]])

                                    testing_data = xgboost.DMatrix(
                                        data=df,
                                    )

                                    model = xgboost.Booster(model_file=model_path)

                                    predictions = model.predict(testing_data)

                                    Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                                    numpy.savetxt(predictions_path, predictions)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost predict', description='Make predictions using a trained XGBoost model.\n\n    Args:\n        data_path: Path for the feature data in CSV format.\n        model_path: Path for the trained model in binary XGBoost format.\n        predictions_path: Output path for the predictions.\n        label_column: Column containing the label data.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_predict(**_parsed_args)
                            args:
                              - '--data'
                              - inputPath: data
                              - '--model'
                              - inputPath: model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - '--predictions'
                              - outputPath: predictions
                    arguments:
                      data:
                        taskOutput:
                          outputName: test_2
                          taskId: Split table into folds
                          type: CSV
                      model:
                        taskOutput:
                          outputName: model
                          taskId: Xgboost train 2
                          type: XGBoostModel
                      label_column:
                        graphInput:
                          inputName: label_column
                  Pandas Transform DataFrame in CSV format 2:
                    componentRef:
                      digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml
                      spec:
                        name: Pandas Transform DataFrame in CSV format
                        description: |-
                          Transform DataFrame loaded from a CSV file.

                              Inputs:
                                  table: Table to transform.
                                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                      The DataFrame variable is called "df".
                                      Examples:
                                      - `df['prod'] = df['X'] * df['Y']`
                                      - `df = df[['X', 'prod']]`
                                      - `df.insert(0, "is_positive", df["X"] > 0)`

                              Outputs:
                                  transformed_table: Transformed table.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                            type: CSV
                          - name: transform_code
                            type: PythonCode
                        outputs:
                          - name: transformed_table
                            type: CSV
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def Pandas_Transform_DataFrame_in_CSV_format(
                                    table_path,
                                    transformed_table_path,
                                    transform_code,
                                ):
                                    '''Transform DataFrame loaded from a CSV file.

                                    Inputs:
                                        table: Table to transform.
                                        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                            The DataFrame variable is called "df".
                                            Examples:
                                            - `df['prod'] = df['X'] * df['Y']`
                                            - `df = df[['X', 'prod']]`
                                            - `df.insert(0, "is_positive", df["X"] > 0)`

                                    Outputs:
                                        transformed_table: Transformed table.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas

                                    df = pandas.read_csv(
                                        table_path,
                                    )
                                    # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                                    namespace = locals()
                                    exec(transform_code, namespace)
                                    namespace['df'].to_csv(
                                        transformed_table_path,
                                        index=False,
                                    )

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
                            args:
                              - '--table'
                              - inputPath: table
                              - '--transform-code'
                              - inputValue: transform_code
                              - '--transformed-table'
                              - outputPath: transformed_table
                    arguments:
                      table:
                        taskOutput:
                          outputName: test_2
                          taskId: Split table into folds
                          type: CSV
                      transform_code: df = df[["tips"]]
                  Remove header 2:
                    componentRef:
                      digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml
                      spec:
                        name: Remove header
                        description: Remove the header line from CSV and TSV data (unconditionally)
                        metadata:
                          annotations:
                            author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                        outputs:
                          - name: table
                        implementation:
                          container:
                            image: alpine
                            command:
                              - sh
                              - '-exc'
                              - |
                                mkdir -p "$(dirname "$1")"
                                tail -n +2 <"$0" >"$1"
                              - inputPath: table
                              - outputPath: table
                    arguments:
                      table:
                        taskOutput:
                          outputName: transformed_table
                          taskId: Pandas Transform DataFrame in CSV format 2
                          type: CSV
                  Calculate regression metrics from csv 2:
                    componentRef:
                      digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml
                      spec:
                        name: Calculate regression metrics from csv
                        description: |-
                          Calculates regression metrics.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: true_values
                          - name: predicted_values
                        outputs:
                          - name: number_of_items
                            type: Integer
                          - name: max_absolute_error
                            type: Float
                          - name: mean_absolute_error
                            type: Float
                          - name: mean_squared_error
                            type: Float
                          - name: root_mean_squared_error
                            type: Float
                          - name: metrics
                            type: JsonObject
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def calculate_regression_metrics_from_csv(
                                    true_values_path,
                                    predicted_values_path,
                                ):
                                    '''Calculates regression metrics.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import math
                                    import numpy

                                    true_values = numpy.loadtxt(true_values_path, dtype=numpy.float64)
                                    predicted_values = numpy.loadtxt(predicted_values_path, dtype=numpy.float64)

                                    if len(predicted_values.shape) != 1:
                                        raise NotImplemented('Only single prediction values are supported.')
                                    if len(true_values.shape) != 1:
                                        raise NotImplemented('Only single true values are supported.')

                                    if predicted_values.shape != true_values.shape:
                                        raise ValueError('Input shapes are different: {} != {}'.format(predicted_values.shape, true_values.shape))

                                    number_of_items = true_values.size
                                    errors = (true_values - predicted_values)
                                    abs_errors = numpy.abs(errors)
                                    squared_errors = errors ** 2
                                    max_absolute_error = numpy.max(abs_errors)
                                    mean_absolute_error = numpy.average(abs_errors)
                                    mean_squared_error = numpy.average(squared_errors)
                                    root_mean_squared_error = math.sqrt(mean_squared_error)
                                    metrics = dict(
                                        number_of_items=number_of_items,
                                        max_absolute_error=max_absolute_error,
                                        mean_absolute_error=mean_absolute_error,
                                        mean_squared_error=mean_squared_error,
                                        root_mean_squared_error=root_mean_squared_error,
                                    )

                                    return (
                                        number_of_items,
                                        max_absolute_error,
                                        mean_absolute_error,
                                        mean_squared_error,
                                        root_mean_squared_error,
                                        metrics,
                                    )

                                def _serialize_json(obj) -> str:
                                    if isinstance(obj, str):
                                        return obj
                                    import json
                                    def default_serializer(obj):
                                        if hasattr(obj, 'to_struct'):
                                            return obj.to_struct()
                                        else:
                                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                                def _serialize_float(float_value: float) -> str:
                                    if isinstance(float_value, str):
                                        return float_value
                                    if not isinstance(float_value, (float, int)):
                                        raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
                                    return str(float_value)

                                def _serialize_int(int_value: int) -> str:
                                    if isinstance(int_value, str):
                                        return int_value
                                    if not isinstance(int_value, int):
                                        raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
                                    return str(int_value)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Calculate regression metrics from csv', description='Calculates regression metrics.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--true-values", dest="true_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--predicted-values", dest="predicted_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=6)
                                _parsed_args = vars(_parser.parse_args())
                                _output_files = _parsed_args.pop("_output_paths", [])

                                _outputs = calculate_regression_metrics_from_csv(**_parsed_args)

                                _output_serializers = [
                                    _serialize_int,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_json,

                                ]

                                import os
                                for idx, output_file in enumerate(_output_files):
                                    try:
                                        os.makedirs(os.path.dirname(output_file))
                                    except OSError:
                                        pass
                                    with open(output_file, 'w') as f:
                                        f.write(_output_serializers[idx](_outputs[idx]))
                            args:
                              - '--true-values'
                              - inputPath: true_values
                              - '--predicted-values'
                              - inputPath: predicted_values
                              - '----output-paths'
                              - outputPath: number_of_items
                              - outputPath: max_absolute_error
                              - outputPath: mean_absolute_error
                              - outputPath: mean_squared_error
                              - outputPath: root_mean_squared_error
                              - outputPath: metrics
                    arguments:
                      true_values:
                        taskOutput:
                          outputName: table
                          taskId: Remove header 2
                      predicted_values:
                        taskOutput:
                          outputName: predictions
                          taskId: Xgboost predict 2
                          type: Text
                  Xgboost train 3:
                    componentRef:
                      digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml
                      spec:
                        name: Xgboost train
                        description: |-
                          Train an XGBoost model.

                              Args:
                                  training_data_path: Path for the training data in CSV format.
                                  model_path: Output path for the trained model in binary XGBoost format.
                                  model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                  starting_model_path: Path for the existing trained model to start from.
                                  label_column: Column containing the label data.
                                  num_boost_rounds: Number of boosting iterations.
                                  booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                  objective: The learning task and the corresponding learning objective.
                                      See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                      The most common values are:
                                      "reg:squarederror" - Regression with squared loss (default).
                                      "reg:logistic" - Logistic regression.
                                      "binary:logistic" - Logistic regression for binary classification, output probability.
                                      "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                      "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                      "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: training_data
                            type: CSV
                          - name: starting_model
                            type: XGBoostModel
                            optional: true
                          - name: label_column
                            type: Integer
                            default: '0'
                            optional: true
                          - name: num_iterations
                            type: Integer
                            default: '10'
                            optional: true
                          - name: booster_params
                            type: JsonObject
                            optional: true
                          - name: objective
                            type: String
                            default: reg:squarederror
                            optional: true
                          - name: booster
                            type: String
                            default: gbtree
                            optional: true
                          - name: learning_rate
                            type: Float
                            default: '0.3'
                            optional: true
                          - name: min_split_loss
                            type: Float
                            default: '0'
                            optional: true
                          - name: max_depth
                            type: Integer
                            default: '6'
                            optional: true
                        outputs:
                          - name: model
                            type: XGBoostModel
                          - name: model_config
                            type: XGBoostModelConfig
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_train(
                                    training_data_path,  # Also supports LibSVM
                                    model_path,
                                    model_config_path,
                                    starting_model_path = None,

                                    label_column = 0,
                                    num_iterations = 10,
                                    booster_params = None,

                                    # Booster parameters
                                    objective = 'reg:squarederror',
                                    booster = 'gbtree',
                                    learning_rate = 0.3,
                                    min_split_loss = 0,
                                    max_depth = 6,
                                ):
                                    '''Train an XGBoost model.

                                    Args:
                                        training_data_path: Path for the training data in CSV format.
                                        model_path: Output path for the trained model in binary XGBoost format.
                                        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                        starting_model_path: Path for the existing trained model to start from.
                                        label_column: Column containing the label data.
                                        num_boost_rounds: Number of boosting iterations.
                                        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                        objective: The learning task and the corresponding learning objective.
                                            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                            The most common values are:
                                            "reg:squarederror" - Regression with squared loss (default).
                                            "reg:logistic" - Logistic regression.
                                            "binary:logistic" - Logistic regression for binary classification, output probability.
                                            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        training_data_path,
                                    )

                                    training_data = xgboost.DMatrix(
                                        data=df.drop(columns=[df.columns[label_column]]),
                                        label=df[df.columns[label_column]],
                                    )

                                    booster_params = booster_params or {}
                                    booster_params.setdefault('objective', objective)
                                    booster_params.setdefault('booster', booster)
                                    booster_params.setdefault('learning_rate', learning_rate)
                                    booster_params.setdefault('min_split_loss', min_split_loss)
                                    booster_params.setdefault('max_depth', max_depth)

                                    starting_model = None
                                    if starting_model_path:
                                        starting_model = xgboost.Booster(model_file=starting_model_path)

                                    model = xgboost.train(
                                        params=booster_params,
                                        dtrain=training_data,
                                        num_boost_round=num_iterations,
                                        xgb_model=starting_model
                                    )

                                    # Saving the model in binary format
                                    model.save_model(model_path)

                                    model_config_str = model.save_config()
                                    with open(model_config_path, 'w') as model_config_file:
                                        model_config_file.write(model_config_str)

                                import json
                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost train', description='Train an XGBoost model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary XGBoost format.\n        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n        num_boost_rounds: Number of boosting iterations.\n        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n        objective: The learning task and the corresponding learning objective.\n            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n            The most common values are:\n            "reg:squarederror" - Regression with squared loss (default).\n            "reg:logistic" - Logistic regression.\n            "binary:logistic" - Logistic regression for binary classification, output probability.\n            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation\n            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_train(**_parsed_args)
                            args:
                              - '--training-data'
                              - inputPath: training_data
                              - if:
                                  cond:
                                    isPresent: starting_model
                                  then:
                                    - '--starting-model'
                                    - inputPath: starting_model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - if:
                                  cond:
                                    isPresent: num_iterations
                                  then:
                                    - '--num-iterations'
                                    - inputValue: num_iterations
                              - if:
                                  cond:
                                    isPresent: booster_params
                                  then:
                                    - '--booster-params'
                                    - inputValue: booster_params
                              - if:
                                  cond:
                                    isPresent: objective
                                  then:
                                    - '--objective'
                                    - inputValue: objective
                              - if:
                                  cond:
                                    isPresent: booster
                                  then:
                                    - '--booster'
                                    - inputValue: booster
                              - if:
                                  cond:
                                    isPresent: learning_rate
                                  then:
                                    - '--learning-rate'
                                    - inputValue: learning_rate
                              - if:
                                  cond:
                                    isPresent: min_split_loss
                                  then:
                                    - '--min-split-loss'
                                    - inputValue: min_split_loss
                              - if:
                                  cond:
                                    isPresent: max_depth
                                  then:
                                    - '--max-depth'
                                    - inputValue: max_depth
                              - '--model'
                              - outputPath: model
                              - '--model-config'
                              - outputPath: model_config
                    arguments:
                      training_data:
                        taskOutput:
                          outputName: train_3
                          taskId: Split table into folds
                          type: CSV
                      label_column:
                        graphInput:
                          inputName: label_column
                      num_iterations:
                        graphInput:
                          inputName: num_iterations
                      objective:
                        graphInput:
                          inputName: objective
                  Xgboost predict 3:
                    componentRef:
                      digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml
                      spec:
                        name: Xgboost predict
                        description: |-
                          Make predictions using a trained XGBoost model.

                              Args:
                                  data_path: Path for the feature data in CSV format.
                                  model_path: Path for the trained model in binary XGBoost format.
                                  predictions_path: Output path for the predictions.
                                  label_column: Column containing the label data.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: data
                            type: CSV
                          - name: model
                            type: XGBoostModel
                          - name: label_column
                            type: Integer
                            optional: true
                        outputs:
                          - name: predictions
                            type: Text
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_predict(
                                    data_path,  # Also supports LibSVM
                                    model_path,
                                    predictions_path,
                                    label_column = None,
                                ):
                                    '''Make predictions using a trained XGBoost model.

                                    Args:
                                        data_path: Path for the feature data in CSV format.
                                        model_path: Path for the trained model in binary XGBoost format.
                                        predictions_path: Output path for the predictions.
                                        label_column: Column containing the label data.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    from pathlib import Path

                                    import numpy
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        data_path,
                                    )

                                    if label_column is not None:
                                        df = df.drop(columns=[df.columns[label_column]])

                                    testing_data = xgboost.DMatrix(
                                        data=df,
                                    )

                                    model = xgboost.Booster(model_file=model_path)

                                    predictions = model.predict(testing_data)

                                    Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                                    numpy.savetxt(predictions_path, predictions)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost predict', description='Make predictions using a trained XGBoost model.\n\n    Args:\n        data_path: Path for the feature data in CSV format.\n        model_path: Path for the trained model in binary XGBoost format.\n        predictions_path: Output path for the predictions.\n        label_column: Column containing the label data.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_predict(**_parsed_args)
                            args:
                              - '--data'
                              - inputPath: data
                              - '--model'
                              - inputPath: model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - '--predictions'
                              - outputPath: predictions
                    arguments:
                      data:
                        taskOutput:
                          outputName: test_3
                          taskId: Split table into folds
                          type: CSV
                      model:
                        taskOutput:
                          outputName: model
                          taskId: Xgboost train 3
                          type: XGBoostModel
                      label_column:
                        graphInput:
                          inputName: label_column
                  Pandas Transform DataFrame in CSV format 3:
                    componentRef:
                      digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml
                      spec:
                        name: Pandas Transform DataFrame in CSV format
                        description: |-
                          Transform DataFrame loaded from a CSV file.

                              Inputs:
                                  table: Table to transform.
                                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                      The DataFrame variable is called "df".
                                      Examples:
                                      - `df['prod'] = df['X'] * df['Y']`
                                      - `df = df[['X', 'prod']]`
                                      - `df.insert(0, "is_positive", df["X"] > 0)`

                              Outputs:
                                  transformed_table: Transformed table.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                            type: CSV
                          - name: transform_code
                            type: PythonCode
                        outputs:
                          - name: transformed_table
                            type: CSV
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def Pandas_Transform_DataFrame_in_CSV_format(
                                    table_path,
                                    transformed_table_path,
                                    transform_code,
                                ):
                                    '''Transform DataFrame loaded from a CSV file.

                                    Inputs:
                                        table: Table to transform.
                                        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                            The DataFrame variable is called "df".
                                            Examples:
                                            - `df['prod'] = df['X'] * df['Y']`
                                            - `df = df[['X', 'prod']]`
                                            - `df.insert(0, "is_positive", df["X"] > 0)`

                                    Outputs:
                                        transformed_table: Transformed table.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas

                                    df = pandas.read_csv(
                                        table_path,
                                    )
                                    # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                                    namespace = locals()
                                    exec(transform_code, namespace)
                                    namespace['df'].to_csv(
                                        transformed_table_path,
                                        index=False,
                                    )

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
                            args:
                              - '--table'
                              - inputPath: table
                              - '--transform-code'
                              - inputValue: transform_code
                              - '--transformed-table'
                              - outputPath: transformed_table
                    arguments:
                      table:
                        taskOutput:
                          outputName: test_3
                          taskId: Split table into folds
                          type: CSV
                      transform_code: df = df[["tips"]]
                  Remove header 3:
                    componentRef:
                      digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml
                      spec:
                        name: Remove header
                        description: Remove the header line from CSV and TSV data (unconditionally)
                        metadata:
                          annotations:
                            author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                        outputs:
                          - name: table
                        implementation:
                          container:
                            image: alpine
                            command:
                              - sh
                              - '-exc'
                              - |
                                mkdir -p "$(dirname "$1")"
                                tail -n +2 <"$0" >"$1"
                              - inputPath: table
                              - outputPath: table
                    arguments:
                      table:
                        taskOutput:
                          outputName: transformed_table
                          taskId: Pandas Transform DataFrame in CSV format 3
                          type: CSV
                  Calculate regression metrics from csv 3:
                    componentRef:
                      digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml
                      spec:
                        name: Calculate regression metrics from csv
                        description: |-
                          Calculates regression metrics.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: true_values
                          - name: predicted_values
                        outputs:
                          - name: number_of_items
                            type: Integer
                          - name: max_absolute_error
                            type: Float
                          - name: mean_absolute_error
                            type: Float
                          - name: mean_squared_error
                            type: Float
                          - name: root_mean_squared_error
                            type: Float
                          - name: metrics
                            type: JsonObject
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def calculate_regression_metrics_from_csv(
                                    true_values_path,
                                    predicted_values_path,
                                ):
                                    '''Calculates regression metrics.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import math
                                    import numpy

                                    true_values = numpy.loadtxt(true_values_path, dtype=numpy.float64)
                                    predicted_values = numpy.loadtxt(predicted_values_path, dtype=numpy.float64)

                                    if len(predicted_values.shape) != 1:
                                        raise NotImplemented('Only single prediction values are supported.')
                                    if len(true_values.shape) != 1:
                                        raise NotImplemented('Only single true values are supported.')

                                    if predicted_values.shape != true_values.shape:
                                        raise ValueError('Input shapes are different: {} != {}'.format(predicted_values.shape, true_values.shape))

                                    number_of_items = true_values.size
                                    errors = (true_values - predicted_values)
                                    abs_errors = numpy.abs(errors)
                                    squared_errors = errors ** 2
                                    max_absolute_error = numpy.max(abs_errors)
                                    mean_absolute_error = numpy.average(abs_errors)
                                    mean_squared_error = numpy.average(squared_errors)
                                    root_mean_squared_error = math.sqrt(mean_squared_error)
                                    metrics = dict(
                                        number_of_items=number_of_items,
                                        max_absolute_error=max_absolute_error,
                                        mean_absolute_error=mean_absolute_error,
                                        mean_squared_error=mean_squared_error,
                                        root_mean_squared_error=root_mean_squared_error,
                                    )

                                    return (
                                        number_of_items,
                                        max_absolute_error,
                                        mean_absolute_error,
                                        mean_squared_error,
                                        root_mean_squared_error,
                                        metrics,
                                    )

                                def _serialize_json(obj) -> str:
                                    if isinstance(obj, str):
                                        return obj
                                    import json
                                    def default_serializer(obj):
                                        if hasattr(obj, 'to_struct'):
                                            return obj.to_struct()
                                        else:
                                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                                def _serialize_float(float_value: float) -> str:
                                    if isinstance(float_value, str):
                                        return float_value
                                    if not isinstance(float_value, (float, int)):
                                        raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
                                    return str(float_value)

                                def _serialize_int(int_value: int) -> str:
                                    if isinstance(int_value, str):
                                        return int_value
                                    if not isinstance(int_value, int):
                                        raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
                                    return str(int_value)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Calculate regression metrics from csv', description='Calculates regression metrics.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--true-values", dest="true_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--predicted-values", dest="predicted_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=6)
                                _parsed_args = vars(_parser.parse_args())
                                _output_files = _parsed_args.pop("_output_paths", [])

                                _outputs = calculate_regression_metrics_from_csv(**_parsed_args)

                                _output_serializers = [
                                    _serialize_int,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_json,

                                ]

                                import os
                                for idx, output_file in enumerate(_output_files):
                                    try:
                                        os.makedirs(os.path.dirname(output_file))
                                    except OSError:
                                        pass
                                    with open(output_file, 'w') as f:
                                        f.write(_output_serializers[idx](_outputs[idx]))
                            args:
                              - '--true-values'
                              - inputPath: true_values
                              - '--predicted-values'
                              - inputPath: predicted_values
                              - '----output-paths'
                              - outputPath: number_of_items
                              - outputPath: max_absolute_error
                              - outputPath: mean_absolute_error
                              - outputPath: mean_squared_error
                              - outputPath: root_mean_squared_error
                              - outputPath: metrics
                    arguments:
                      true_values:
                        taskOutput:
                          outputName: table
                          taskId: Remove header 3
                      predicted_values:
                        taskOutput:
                          outputName: predictions
                          taskId: Xgboost predict 3
                          type: Text
                  Xgboost train 4:
                    componentRef:
                      digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml
                      spec:
                        name: Xgboost train
                        description: |-
                          Train an XGBoost model.

                              Args:
                                  training_data_path: Path for the training data in CSV format.
                                  model_path: Output path for the trained model in binary XGBoost format.
                                  model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                  starting_model_path: Path for the existing trained model to start from.
                                  label_column: Column containing the label data.
                                  num_boost_rounds: Number of boosting iterations.
                                  booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                  objective: The learning task and the corresponding learning objective.
                                      See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                      The most common values are:
                                      "reg:squarederror" - Regression with squared loss (default).
                                      "reg:logistic" - Logistic regression.
                                      "binary:logistic" - Logistic regression for binary classification, output probability.
                                      "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                      "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                      "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: training_data
                            type: CSV
                          - name: starting_model
                            type: XGBoostModel
                            optional: true
                          - name: label_column
                            type: Integer
                            default: '0'
                            optional: true
                          - name: num_iterations
                            type: Integer
                            default: '10'
                            optional: true
                          - name: booster_params
                            type: JsonObject
                            optional: true
                          - name: objective
                            type: String
                            default: reg:squarederror
                            optional: true
                          - name: booster
                            type: String
                            default: gbtree
                            optional: true
                          - name: learning_rate
                            type: Float
                            default: '0.3'
                            optional: true
                          - name: min_split_loss
                            type: Float
                            default: '0'
                            optional: true
                          - name: max_depth
                            type: Integer
                            default: '6'
                            optional: true
                        outputs:
                          - name: model
                            type: XGBoostModel
                          - name: model_config
                            type: XGBoostModelConfig
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_train(
                                    training_data_path,  # Also supports LibSVM
                                    model_path,
                                    model_config_path,
                                    starting_model_path = None,

                                    label_column = 0,
                                    num_iterations = 10,
                                    booster_params = None,

                                    # Booster parameters
                                    objective = 'reg:squarederror',
                                    booster = 'gbtree',
                                    learning_rate = 0.3,
                                    min_split_loss = 0,
                                    max_depth = 6,
                                ):
                                    '''Train an XGBoost model.

                                    Args:
                                        training_data_path: Path for the training data in CSV format.
                                        model_path: Output path for the trained model in binary XGBoost format.
                                        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                        starting_model_path: Path for the existing trained model to start from.
                                        label_column: Column containing the label data.
                                        num_boost_rounds: Number of boosting iterations.
                                        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                        objective: The learning task and the corresponding learning objective.
                                            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                            The most common values are:
                                            "reg:squarederror" - Regression with squared loss (default).
                                            "reg:logistic" - Logistic regression.
                                            "binary:logistic" - Logistic regression for binary classification, output probability.
                                            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        training_data_path,
                                    )

                                    training_data = xgboost.DMatrix(
                                        data=df.drop(columns=[df.columns[label_column]]),
                                        label=df[df.columns[label_column]],
                                    )

                                    booster_params = booster_params or {}
                                    booster_params.setdefault('objective', objective)
                                    booster_params.setdefault('booster', booster)
                                    booster_params.setdefault('learning_rate', learning_rate)
                                    booster_params.setdefault('min_split_loss', min_split_loss)
                                    booster_params.setdefault('max_depth', max_depth)

                                    starting_model = None
                                    if starting_model_path:
                                        starting_model = xgboost.Booster(model_file=starting_model_path)

                                    model = xgboost.train(
                                        params=booster_params,
                                        dtrain=training_data,
                                        num_boost_round=num_iterations,
                                        xgb_model=starting_model
                                    )

                                    # Saving the model in binary format
                                    model.save_model(model_path)

                                    model_config_str = model.save_config()
                                    with open(model_config_path, 'w') as model_config_file:
                                        model_config_file.write(model_config_str)

                                import json
                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost train', description='Train an XGBoost model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary XGBoost format.\n        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n        num_boost_rounds: Number of boosting iterations.\n        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n        objective: The learning task and the corresponding learning objective.\n            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n            The most common values are:\n            "reg:squarederror" - Regression with squared loss (default).\n            "reg:logistic" - Logistic regression.\n            "binary:logistic" - Logistic regression for binary classification, output probability.\n            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation\n            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_train(**_parsed_args)
                            args:
                              - '--training-data'
                              - inputPath: training_data
                              - if:
                                  cond:
                                    isPresent: starting_model
                                  then:
                                    - '--starting-model'
                                    - inputPath: starting_model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - if:
                                  cond:
                                    isPresent: num_iterations
                                  then:
                                    - '--num-iterations'
                                    - inputValue: num_iterations
                              - if:
                                  cond:
                                    isPresent: booster_params
                                  then:
                                    - '--booster-params'
                                    - inputValue: booster_params
                              - if:
                                  cond:
                                    isPresent: objective
                                  then:
                                    - '--objective'
                                    - inputValue: objective
                              - if:
                                  cond:
                                    isPresent: booster
                                  then:
                                    - '--booster'
                                    - inputValue: booster
                              - if:
                                  cond:
                                    isPresent: learning_rate
                                  then:
                                    - '--learning-rate'
                                    - inputValue: learning_rate
                              - if:
                                  cond:
                                    isPresent: min_split_loss
                                  then:
                                    - '--min-split-loss'
                                    - inputValue: min_split_loss
                              - if:
                                  cond:
                                    isPresent: max_depth
                                  then:
                                    - '--max-depth'
                                    - inputValue: max_depth
                              - '--model'
                              - outputPath: model
                              - '--model-config'
                              - outputPath: model_config
                    arguments:
                      training_data:
                        taskOutput:
                          outputName: train_4
                          taskId: Split table into folds
                          type: CSV
                      label_column:
                        graphInput:
                          inputName: label_column
                      num_iterations:
                        graphInput:
                          inputName: num_iterations
                      objective:
                        graphInput:
                          inputName: objective
                  Xgboost predict 4:
                    componentRef:
                      digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml
                      spec:
                        name: Xgboost predict
                        description: |-
                          Make predictions using a trained XGBoost model.

                              Args:
                                  data_path: Path for the feature data in CSV format.
                                  model_path: Path for the trained model in binary XGBoost format.
                                  predictions_path: Output path for the predictions.
                                  label_column: Column containing the label data.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: data
                            type: CSV
                          - name: model
                            type: XGBoostModel
                          - name: label_column
                            type: Integer
                            optional: true
                        outputs:
                          - name: predictions
                            type: Text
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_predict(
                                    data_path,  # Also supports LibSVM
                                    model_path,
                                    predictions_path,
                                    label_column = None,
                                ):
                                    '''Make predictions using a trained XGBoost model.

                                    Args:
                                        data_path: Path for the feature data in CSV format.
                                        model_path: Path for the trained model in binary XGBoost format.
                                        predictions_path: Output path for the predictions.
                                        label_column: Column containing the label data.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    from pathlib import Path

                                    import numpy
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        data_path,
                                    )

                                    if label_column is not None:
                                        df = df.drop(columns=[df.columns[label_column]])

                                    testing_data = xgboost.DMatrix(
                                        data=df,
                                    )

                                    model = xgboost.Booster(model_file=model_path)

                                    predictions = model.predict(testing_data)

                                    Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                                    numpy.savetxt(predictions_path, predictions)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost predict', description='Make predictions using a trained XGBoost model.\n\n    Args:\n        data_path: Path for the feature data in CSV format.\n        model_path: Path for the trained model in binary XGBoost format.\n        predictions_path: Output path for the predictions.\n        label_column: Column containing the label data.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_predict(**_parsed_args)
                            args:
                              - '--data'
                              - inputPath: data
                              - '--model'
                              - inputPath: model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - '--predictions'
                              - outputPath: predictions
                    arguments:
                      data:
                        taskOutput:
                          outputName: test_4
                          taskId: Split table into folds
                          type: CSV
                      model:
                        taskOutput:
                          outputName: model
                          taskId: Xgboost train 4
                          type: XGBoostModel
                      label_column:
                        graphInput:
                          inputName: label_column
                  Pandas Transform DataFrame in CSV format 4:
                    componentRef:
                      digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml
                      spec:
                        name: Pandas Transform DataFrame in CSV format
                        description: |-
                          Transform DataFrame loaded from a CSV file.

                              Inputs:
                                  table: Table to transform.
                                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                      The DataFrame variable is called "df".
                                      Examples:
                                      - `df['prod'] = df['X'] * df['Y']`
                                      - `df = df[['X', 'prod']]`
                                      - `df.insert(0, "is_positive", df["X"] > 0)`

                              Outputs:
                                  transformed_table: Transformed table.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                            type: CSV
                          - name: transform_code
                            type: PythonCode
                        outputs:
                          - name: transformed_table
                            type: CSV
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def Pandas_Transform_DataFrame_in_CSV_format(
                                    table_path,
                                    transformed_table_path,
                                    transform_code,
                                ):
                                    '''Transform DataFrame loaded from a CSV file.

                                    Inputs:
                                        table: Table to transform.
                                        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                            The DataFrame variable is called "df".
                                            Examples:
                                            - `df['prod'] = df['X'] * df['Y']`
                                            - `df = df[['X', 'prod']]`
                                            - `df.insert(0, "is_positive", df["X"] > 0)`

                                    Outputs:
                                        transformed_table: Transformed table.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas

                                    df = pandas.read_csv(
                                        table_path,
                                    )
                                    # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                                    namespace = locals()
                                    exec(transform_code, namespace)
                                    namespace['df'].to_csv(
                                        transformed_table_path,
                                        index=False,
                                    )

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
                            args:
                              - '--table'
                              - inputPath: table
                              - '--transform-code'
                              - inputValue: transform_code
                              - '--transformed-table'
                              - outputPath: transformed_table
                    arguments:
                      table:
                        taskOutput:
                          outputName: test_4
                          taskId: Split table into folds
                          type: CSV
                      transform_code: df = df[["tips"]]
                  Remove header 4:
                    componentRef:
                      digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml
                      spec:
                        name: Remove header
                        description: Remove the header line from CSV and TSV data (unconditionally)
                        metadata:
                          annotations:
                            author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                        outputs:
                          - name: table
                        implementation:
                          container:
                            image: alpine
                            command:
                              - sh
                              - '-exc'
                              - |
                                mkdir -p "$(dirname "$1")"
                                tail -n +2 <"$0" >"$1"
                              - inputPath: table
                              - outputPath: table
                    arguments:
                      table:
                        taskOutput:
                          outputName: transformed_table
                          taskId: Pandas Transform DataFrame in CSV format 4
                          type: CSV
                  Calculate regression metrics from csv 4:
                    componentRef:
                      digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml
                      spec:
                        name: Calculate regression metrics from csv
                        description: |-
                          Calculates regression metrics.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: true_values
                          - name: predicted_values
                        outputs:
                          - name: number_of_items
                            type: Integer
                          - name: max_absolute_error
                            type: Float
                          - name: mean_absolute_error
                            type: Float
                          - name: mean_squared_error
                            type: Float
                          - name: root_mean_squared_error
                            type: Float
                          - name: metrics
                            type: JsonObject
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def calculate_regression_metrics_from_csv(
                                    true_values_path,
                                    predicted_values_path,
                                ):
                                    '''Calculates regression metrics.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import math
                                    import numpy

                                    true_values = numpy.loadtxt(true_values_path, dtype=numpy.float64)
                                    predicted_values = numpy.loadtxt(predicted_values_path, dtype=numpy.float64)

                                    if len(predicted_values.shape) != 1:
                                        raise NotImplemented('Only single prediction values are supported.')
                                    if len(true_values.shape) != 1:
                                        raise NotImplemented('Only single true values are supported.')

                                    if predicted_values.shape != true_values.shape:
                                        raise ValueError('Input shapes are different: {} != {}'.format(predicted_values.shape, true_values.shape))

                                    number_of_items = true_values.size
                                    errors = (true_values - predicted_values)
                                    abs_errors = numpy.abs(errors)
                                    squared_errors = errors ** 2
                                    max_absolute_error = numpy.max(abs_errors)
                                    mean_absolute_error = numpy.average(abs_errors)
                                    mean_squared_error = numpy.average(squared_errors)
                                    root_mean_squared_error = math.sqrt(mean_squared_error)
                                    metrics = dict(
                                        number_of_items=number_of_items,
                                        max_absolute_error=max_absolute_error,
                                        mean_absolute_error=mean_absolute_error,
                                        mean_squared_error=mean_squared_error,
                                        root_mean_squared_error=root_mean_squared_error,
                                    )

                                    return (
                                        number_of_items,
                                        max_absolute_error,
                                        mean_absolute_error,
                                        mean_squared_error,
                                        root_mean_squared_error,
                                        metrics,
                                    )

                                def _serialize_json(obj) -> str:
                                    if isinstance(obj, str):
                                        return obj
                                    import json
                                    def default_serializer(obj):
                                        if hasattr(obj, 'to_struct'):
                                            return obj.to_struct()
                                        else:
                                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                                def _serialize_float(float_value: float) -> str:
                                    if isinstance(float_value, str):
                                        return float_value
                                    if not isinstance(float_value, (float, int)):
                                        raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
                                    return str(float_value)

                                def _serialize_int(int_value: int) -> str:
                                    if isinstance(int_value, str):
                                        return int_value
                                    if not isinstance(int_value, int):
                                        raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
                                    return str(int_value)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Calculate regression metrics from csv', description='Calculates regression metrics.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--true-values", dest="true_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--predicted-values", dest="predicted_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=6)
                                _parsed_args = vars(_parser.parse_args())
                                _output_files = _parsed_args.pop("_output_paths", [])

                                _outputs = calculate_regression_metrics_from_csv(**_parsed_args)

                                _output_serializers = [
                                    _serialize_int,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_json,

                                ]

                                import os
                                for idx, output_file in enumerate(_output_files):
                                    try:
                                        os.makedirs(os.path.dirname(output_file))
                                    except OSError:
                                        pass
                                    with open(output_file, 'w') as f:
                                        f.write(_output_serializers[idx](_outputs[idx]))
                            args:
                              - '--true-values'
                              - inputPath: true_values
                              - '--predicted-values'
                              - inputPath: predicted_values
                              - '----output-paths'
                              - outputPath: number_of_items
                              - outputPath: max_absolute_error
                              - outputPath: mean_absolute_error
                              - outputPath: mean_squared_error
                              - outputPath: root_mean_squared_error
                              - outputPath: metrics
                    arguments:
                      true_values:
                        taskOutput:
                          outputName: table
                          taskId: Remove header 4
                      predicted_values:
                        taskOutput:
                          outputName: predictions
                          taskId: Xgboost predict 4
                          type: Text
                  Xgboost train 5:
                    componentRef:
                      digest: 09b80053da29f8f51575b42e5d2e8ad4b7bdcc92a02c3744e189b1f597006b38
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Train/component.yaml
                      spec:
                        name: Xgboost train
                        description: |-
                          Train an XGBoost model.

                              Args:
                                  training_data_path: Path for the training data in CSV format.
                                  model_path: Output path for the trained model in binary XGBoost format.
                                  model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                  starting_model_path: Path for the existing trained model to start from.
                                  label_column: Column containing the label data.
                                  num_boost_rounds: Number of boosting iterations.
                                  booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                  objective: The learning task and the corresponding learning objective.
                                      See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                      The most common values are:
                                      "reg:squarederror" - Regression with squared loss (default).
                                      "reg:logistic" - Logistic regression.
                                      "binary:logistic" - Logistic regression for binary classification, output probability.
                                      "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                      "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                      "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: training_data
                            type: CSV
                          - name: starting_model
                            type: XGBoostModel
                            optional: true
                          - name: label_column
                            type: Integer
                            default: '0'
                            optional: true
                          - name: num_iterations
                            type: Integer
                            default: '10'
                            optional: true
                          - name: booster_params
                            type: JsonObject
                            optional: true
                          - name: objective
                            type: String
                            default: reg:squarederror
                            optional: true
                          - name: booster
                            type: String
                            default: gbtree
                            optional: true
                          - name: learning_rate
                            type: Float
                            default: '0.3'
                            optional: true
                          - name: min_split_loss
                            type: Float
                            default: '0'
                            optional: true
                          - name: max_depth
                            type: Integer
                            default: '6'
                            optional: true
                        outputs:
                          - name: model
                            type: XGBoostModel
                          - name: model_config
                            type: XGBoostModelConfig
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_train(
                                    training_data_path,  # Also supports LibSVM
                                    model_path,
                                    model_config_path,
                                    starting_model_path = None,

                                    label_column = 0,
                                    num_iterations = 10,
                                    booster_params = None,

                                    # Booster parameters
                                    objective = 'reg:squarederror',
                                    booster = 'gbtree',
                                    learning_rate = 0.3,
                                    min_split_loss = 0,
                                    max_depth = 6,
                                ):
                                    '''Train an XGBoost model.

                                    Args:
                                        training_data_path: Path for the training data in CSV format.
                                        model_path: Output path for the trained model in binary XGBoost format.
                                        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.
                                        starting_model_path: Path for the existing trained model to start from.
                                        label_column: Column containing the label data.
                                        num_boost_rounds: Number of boosting iterations.
                                        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html
                                        objective: The learning task and the corresponding learning objective.
                                            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters
                                            The most common values are:
                                            "reg:squarederror" - Regression with squared loss (default).
                                            "reg:logistic" - Logistic regression.
                                            "binary:logistic" - Logistic regression for binary classification, output probability.
                                            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation
                                            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized
                                            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        training_data_path,
                                    )

                                    training_data = xgboost.DMatrix(
                                        data=df.drop(columns=[df.columns[label_column]]),
                                        label=df[df.columns[label_column]],
                                    )

                                    booster_params = booster_params or {}
                                    booster_params.setdefault('objective', objective)
                                    booster_params.setdefault('booster', booster)
                                    booster_params.setdefault('learning_rate', learning_rate)
                                    booster_params.setdefault('min_split_loss', min_split_loss)
                                    booster_params.setdefault('max_depth', max_depth)

                                    starting_model = None
                                    if starting_model_path:
                                        starting_model = xgboost.Booster(model_file=starting_model_path)

                                    model = xgboost.train(
                                        params=booster_params,
                                        dtrain=training_data,
                                        num_boost_round=num_iterations,
                                        xgb_model=starting_model
                                    )

                                    # Saving the model in binary format
                                    model.save_model(model_path)

                                    model_config_str = model.save_config()
                                    with open(model_config_path, 'w') as model_config_file:
                                        model_config_file.write(model_config_str)

                                import json
                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost train', description='Train an XGBoost model.\n\n    Args:\n        training_data_path: Path for the training data in CSV format.\n        model_path: Output path for the trained model in binary XGBoost format.\n        model_config_path: Output path for the internal parameter configuration of Booster as a JSON string.\n        starting_model_path: Path for the existing trained model to start from.\n        label_column: Column containing the label data.\n        num_boost_rounds: Number of boosting iterations.\n        booster_params: Parameters for the booster. See https://xgboost.readthedocs.io/en/latest/parameter.html\n        objective: The learning task and the corresponding learning objective.\n            See https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n            The most common values are:\n            "reg:squarederror" - Regression with squared loss (default).\n            "reg:logistic" - Logistic regression.\n            "binary:logistic" - Logistic regression for binary classification, output probability.\n            "binary:logitraw" - Logistic regression for binary classification, output score before logistic transformation\n            "rank:pairwise" - Use LambdaMART to perform pairwise ranking where the pairwise loss is minimized\n            "rank:ndcg" - Use LambdaMART to perform list-wise ranking where Normalized Discounted Cumulative Gain (NDCG) is maximized\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--training-data", dest="training_data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--starting-model", dest="starting_model_path", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--num-iterations", dest="num_iterations", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster-params", dest="booster_params", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--objective", dest="objective", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--booster", dest="booster", type=str, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--learning-rate", dest="learning_rate", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--min-split-loss", dest="min_split_loss", type=float, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--max-depth", dest="max_depth", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model-config", dest="model_config_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_train(**_parsed_args)
                            args:
                              - '--training-data'
                              - inputPath: training_data
                              - if:
                                  cond:
                                    isPresent: starting_model
                                  then:
                                    - '--starting-model'
                                    - inputPath: starting_model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - if:
                                  cond:
                                    isPresent: num_iterations
                                  then:
                                    - '--num-iterations'
                                    - inputValue: num_iterations
                              - if:
                                  cond:
                                    isPresent: booster_params
                                  then:
                                    - '--booster-params'
                                    - inputValue: booster_params
                              - if:
                                  cond:
                                    isPresent: objective
                                  then:
                                    - '--objective'
                                    - inputValue: objective
                              - if:
                                  cond:
                                    isPresent: booster
                                  then:
                                    - '--booster'
                                    - inputValue: booster
                              - if:
                                  cond:
                                    isPresent: learning_rate
                                  then:
                                    - '--learning-rate'
                                    - inputValue: learning_rate
                              - if:
                                  cond:
                                    isPresent: min_split_loss
                                  then:
                                    - '--min-split-loss'
                                    - inputValue: min_split_loss
                              - if:
                                  cond:
                                    isPresent: max_depth
                                  then:
                                    - '--max-depth'
                                    - inputValue: max_depth
                              - '--model'
                              - outputPath: model
                              - '--model-config'
                              - outputPath: model_config
                    arguments:
                      training_data:
                        taskOutput:
                          outputName: train_5
                          taskId: Split table into folds
                          type: CSV
                      label_column:
                        graphInput:
                          inputName: label_column
                      num_iterations:
                        graphInput:
                          inputName: num_iterations
                      objective:
                        graphInput:
                          inputName: objective
                  Xgboost predict 5:
                    componentRef:
                      digest: ecdfaf32cff15b6abc3d0dd80365ce00577f1a19a058fbe201f515431cea1357
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/567c04c51ff00a1ee525b3458425b17adbe3df61/components/XGBoost/Predict/component.yaml
                      spec:
                        name: Xgboost predict
                        description: |-
                          Make predictions using a trained XGBoost model.

                              Args:
                                  data_path: Path for the feature data in CSV format.
                                  model_path: Path for the trained model in binary XGBoost format.
                                  predictions_path: Output path for the predictions.
                                  label_column: Column containing the label data.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: data
                            type: CSV
                          - name: model
                            type: XGBoostModel
                          - name: label_column
                            type: Integer
                            optional: true
                        outputs:
                          - name: predictions
                            type: Text
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'xgboost==1.1.1' 'pandas==1.0.5' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def xgboost_predict(
                                    data_path,  # Also supports LibSVM
                                    model_path,
                                    predictions_path,
                                    label_column = None,
                                ):
                                    '''Make predictions using a trained XGBoost model.

                                    Args:
                                        data_path: Path for the feature data in CSV format.
                                        model_path: Path for the trained model in binary XGBoost format.
                                        predictions_path: Output path for the predictions.
                                        label_column: Column containing the label data.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    from pathlib import Path

                                    import numpy
                                    import pandas
                                    import xgboost

                                    df = pandas.read_csv(
                                        data_path,
                                    )

                                    if label_column is not None:
                                        df = df.drop(columns=[df.columns[label_column]])

                                    testing_data = xgboost.DMatrix(
                                        data=df,
                                    )

                                    model = xgboost.Booster(model_file=model_path)

                                    predictions = model.predict(testing_data)

                                    Path(predictions_path).parent.mkdir(parents=True, exist_ok=True)
                                    numpy.savetxt(predictions_path, predictions)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Xgboost predict', description='Make predictions using a trained XGBoost model.\n\n    Args:\n        data_path: Path for the feature data in CSV format.\n        model_path: Path for the trained model in binary XGBoost format.\n        predictions_path: Output path for the predictions.\n        label_column: Column containing the label data.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--data", dest="data_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--model", dest="model_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--label-column", dest="label_column", type=int, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--predictions", dest="predictions_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = xgboost_predict(**_parsed_args)
                            args:
                              - '--data'
                              - inputPath: data
                              - '--model'
                              - inputPath: model
                              - if:
                                  cond:
                                    isPresent: label_column
                                  then:
                                    - '--label-column'
                                    - inputValue: label_column
                              - '--predictions'
                              - outputPath: predictions
                    arguments:
                      data:
                        taskOutput:
                          outputName: test_5
                          taskId: Split table into folds
                          type: CSV
                      model:
                        taskOutput:
                          outputName: model
                          taskId: Xgboost train 5
                          type: XGBoostModel
                      label_column:
                        graphInput:
                          inputName: label_column
                  Pandas Transform DataFrame in CSV format 5:
                    componentRef:
                      digest: 58dc88349157bf128021708c316ce4eb60bc1de0a5a7dd3af45fabac3276d510
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/6162d55998b176b50267d351241100bb0ee715bc/components/pandas/Transform_DataFrame/in_CSV_format/component.yaml
                      spec:
                        name: Pandas Transform DataFrame in CSV format
                        description: |-
                          Transform DataFrame loaded from a CSV file.

                              Inputs:
                                  table: Table to transform.
                                  transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                      The DataFrame variable is called "df".
                                      Examples:
                                      - `df['prod'] = df['X'] * df['Y']`
                                      - `df = df[['X', 'prod']]`
                                      - `df.insert(0, "is_positive", df["X"] > 0)`

                              Outputs:
                                  transformed_table: Transformed table.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                            type: CSV
                          - name: transform_code
                            type: PythonCode
                        outputs:
                          - name: transformed_table
                            type: CSV
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'pandas==1.0.4' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def _make_parent_dirs_and_return_path(file_path: str):
                                    import os
                                    os.makedirs(os.path.dirname(file_path), exist_ok=True)
                                    return file_path

                                def Pandas_Transform_DataFrame_in_CSV_format(
                                    table_path,
                                    transformed_table_path,
                                    transform_code,
                                ):
                                    '''Transform DataFrame loaded from a CSV file.

                                    Inputs:
                                        table: Table to transform.
                                        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.
                                            The DataFrame variable is called "df".
                                            Examples:
                                            - `df['prod'] = df['X'] * df['Y']`
                                            - `df = df[['X', 'prod']]`
                                            - `df.insert(0, "is_positive", df["X"] > 0)`

                                    Outputs:
                                        transformed_table: Transformed table.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import pandas

                                    df = pandas.read_csv(
                                        table_path,
                                    )
                                    # The namespace is needed so that the code can replace `df`. For example df = df[['X']]
                                    namespace = locals()
                                    exec(transform_code, namespace)
                                    namespace['df'].to_csv(
                                        transformed_table_path,
                                        index=False,
                                    )

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Pandas Transform DataFrame in CSV format', description='Transform DataFrame loaded from a CSV file.\n\n    Inputs:\n        table: Table to transform.\n        transform_code: Transformation code. Code is written in Python and can consist of multiple lines.\n            The DataFrame variable is called "df".\n            Examples:\n            - `df[\'prod\'] = df[\'X\'] * df[\'Y\']`\n            - `df = df[[\'X\', \'prod\']]`\n            - `df.insert(0, "is_positive", df["X"] > 0)`\n\n    Outputs:\n        transformed_table: Transformed table.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--table", dest="table_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transform-code", dest="transform_code", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--transformed-table", dest="transformed_table_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
                                _parsed_args = vars(_parser.parse_args())

                                _outputs = Pandas_Transform_DataFrame_in_CSV_format(**_parsed_args)
                            args:
                              - '--table'
                              - inputPath: table
                              - '--transform-code'
                              - inputValue: transform_code
                              - '--transformed-table'
                              - outputPath: transformed_table
                    arguments:
                      table:
                        taskOutput:
                          outputName: test_5
                          taskId: Split table into folds
                          type: CSV
                      transform_code: df = df[["tips"]]
                  Remove header 5:
                    componentRef:
                      digest: ba35ffea863855b956c3c50aefa0420ba3823949a6c059e6e3971cde960dc5a3
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/02c9638287468c849632cf9f7885b51de4c66f86/components/tables/Remove_header/component.yaml
                      spec:
                        name: Remove header
                        description: Remove the header line from CSV and TSV data (unconditionally)
                        metadata:
                          annotations:
                            author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: table
                        outputs:
                          - name: table
                        implementation:
                          container:
                            image: alpine
                            command:
                              - sh
                              - '-exc'
                              - |
                                mkdir -p "$(dirname "$1")"
                                tail -n +2 <"$0" >"$1"
                              - inputPath: table
                              - outputPath: table
                    arguments:
                      table:
                        taskOutput:
                          outputName: transformed_table
                          taskId: Pandas Transform DataFrame in CSV format 5
                          type: CSV
                  Calculate regression metrics from csv 5:
                    componentRef:
                      digest: e3ecbfeb18032820edfee4255e2fb6d15d15ed224e166519d5e528e12053a995
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/7da1ac9464b4b3e7d95919faa2f1107a9635b7e4/components/ml_metrics/Calculate_regression_metrics/from_CSV/component.yaml
                      spec:
                        name: Calculate regression metrics from csv
                        description: |-
                          Calculates regression metrics.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: true_values
                          - name: predicted_values
                        outputs:
                          - name: number_of_items
                            type: Integer
                          - name: max_absolute_error
                            type: Float
                          - name: mean_absolute_error
                            type: Float
                          - name: mean_squared_error
                            type: Float
                          - name: root_mean_squared_error
                            type: Float
                          - name: metrics
                            type: JsonObject
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - sh
                              - '-c'
                              - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location 'numpy==1.19.0' --user) && "$0" "$@"
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def calculate_regression_metrics_from_csv(
                                    true_values_path,
                                    predicted_values_path,
                                ):
                                    '''Calculates regression metrics.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import math
                                    import numpy

                                    true_values = numpy.loadtxt(true_values_path, dtype=numpy.float64)
                                    predicted_values = numpy.loadtxt(predicted_values_path, dtype=numpy.float64)

                                    if len(predicted_values.shape) != 1:
                                        raise NotImplemented('Only single prediction values are supported.')
                                    if len(true_values.shape) != 1:
                                        raise NotImplemented('Only single true values are supported.')

                                    if predicted_values.shape != true_values.shape:
                                        raise ValueError('Input shapes are different: {} != {}'.format(predicted_values.shape, true_values.shape))

                                    number_of_items = true_values.size
                                    errors = (true_values - predicted_values)
                                    abs_errors = numpy.abs(errors)
                                    squared_errors = errors ** 2
                                    max_absolute_error = numpy.max(abs_errors)
                                    mean_absolute_error = numpy.average(abs_errors)
                                    mean_squared_error = numpy.average(squared_errors)
                                    root_mean_squared_error = math.sqrt(mean_squared_error)
                                    metrics = dict(
                                        number_of_items=number_of_items,
                                        max_absolute_error=max_absolute_error,
                                        mean_absolute_error=mean_absolute_error,
                                        mean_squared_error=mean_squared_error,
                                        root_mean_squared_error=root_mean_squared_error,
                                    )

                                    return (
                                        number_of_items,
                                        max_absolute_error,
                                        mean_absolute_error,
                                        mean_squared_error,
                                        root_mean_squared_error,
                                        metrics,
                                    )

                                def _serialize_json(obj) -> str:
                                    if isinstance(obj, str):
                                        return obj
                                    import json
                                    def default_serializer(obj):
                                        if hasattr(obj, 'to_struct'):
                                            return obj.to_struct()
                                        else:
                                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                                def _serialize_float(float_value: float) -> str:
                                    if isinstance(float_value, str):
                                        return float_value
                                    if not isinstance(float_value, (float, int)):
                                        raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
                                    return str(float_value)

                                def _serialize_int(int_value: int) -> str:
                                    if isinstance(int_value, str):
                                        return int_value
                                    if not isinstance(int_value, int):
                                        raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
                                    return str(int_value)

                                import argparse
                                _parser = argparse.ArgumentParser(prog='Calculate regression metrics from csv', description='Calculates regression metrics.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--true-values", dest="true_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--predicted-values", dest="predicted_values_path", type=str, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=6)
                                _parsed_args = vars(_parser.parse_args())
                                _output_files = _parsed_args.pop("_output_paths", [])

                                _outputs = calculate_regression_metrics_from_csv(**_parsed_args)

                                _output_serializers = [
                                    _serialize_int,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_json,

                                ]

                                import os
                                for idx, output_file in enumerate(_output_files):
                                    try:
                                        os.makedirs(os.path.dirname(output_file))
                                    except OSError:
                                        pass
                                    with open(output_file, 'w') as f:
                                        f.write(_output_serializers[idx](_outputs[idx]))
                            args:
                              - '--true-values'
                              - inputPath: true_values
                              - '--predicted-values'
                              - inputPath: predicted_values
                              - '----output-paths'
                              - outputPath: number_of_items
                              - outputPath: max_absolute_error
                              - outputPath: mean_absolute_error
                              - outputPath: mean_squared_error
                              - outputPath: root_mean_squared_error
                              - outputPath: metrics
                    arguments:
                      true_values:
                        taskOutput:
                          outputName: table
                          taskId: Remove header 5
                      predicted_values:
                        taskOutput:
                          outputName: predictions
                          taskId: Xgboost predict 5
                          type: Text
                  Aggregate regression metrics from csv:
                    componentRef:
                      digest: 3e128130521eff8d43764f3dcb037316cdd6490ad2878df5adef416f7c2f3c19
                      url: https://raw.githubusercontent.com/kubeflow/pipelines/7ea9363fe201918d419fecdc00d1275e657ff712/components/ml_metrics/Aggregate_regression_metrics/component.yaml
                      spec:
                        name: Aggregate regression metrics from csv
                        description: |-
                          Calculates regression metrics.

                              Annotations:
                                  author: Alexey Volkov <alexey.volkov@ark-kun.com>
                        inputs:
                          - name: metrics_1
                            type: JsonObject
                          - name: metrics_2
                            type: JsonObject
                            optional: true
                          - name: metrics_3
                            type: JsonObject
                            optional: true
                          - name: metrics_4
                            type: JsonObject
                            optional: true
                          - name: metrics_5
                            type: JsonObject
                            optional: true
                        outputs:
                          - name: number_of_items
                            type: Integer
                          - name: max_absolute_error
                            type: Float
                          - name: mean_absolute_error
                            type: Float
                          - name: mean_squared_error
                            type: Float
                          - name: root_mean_squared_error
                            type: Float
                          - name: metrics
                            type: JsonObject
                        implementation:
                          container:
                            image: python:3.7
                            command:
                              - python3
                              - '-u'
                              - '-c'
                              - |
                                def aggregate_regression_metrics_from_csv(
                                    metrics_1,
                                    metrics_2 = None,
                                    metrics_3 = None,
                                    metrics_4 = None,
                                    metrics_5 = None,
                                ):
                                    '''Calculates regression metrics.

                                    Annotations:
                                        author: Alexey Volkov <alexey.volkov@ark-kun.com>
                                    '''
                                    import math

                                    metrics_dicts = [d for d in [metrics_1, metrics_2, metrics_3, metrics_4, metrics_5] if d is not None]
                                    number_of_items = sum(metrics['number_of_items'] for metrics in metrics_dicts)
                                    max_absolute_error = max(metrics['max_absolute_error'] for metrics in metrics_dicts)
                                    mean_absolute_error = sum(metrics['mean_absolute_error'] * metrics['number_of_items'] for metrics in metrics_dicts) / number_of_items
                                    mean_squared_error = sum(metrics['mean_squared_error'] * metrics['number_of_items'] for metrics in metrics_dicts) / number_of_items
                                    root_mean_squared_error = math.sqrt(mean_squared_error)
                                    metrics = dict(
                                        number_of_items=number_of_items,
                                        max_absolute_error=max_absolute_error,
                                        mean_absolute_error=mean_absolute_error,
                                        mean_squared_error=mean_squared_error,
                                        root_mean_squared_error=root_mean_squared_error,
                                    )

                                    return (
                                        number_of_items,
                                        max_absolute_error,
                                        mean_absolute_error,
                                        mean_squared_error,
                                        root_mean_squared_error,
                                        metrics,
                                    )

                                def _serialize_json(obj) -> str:
                                    if isinstance(obj, str):
                                        return obj
                                    import json
                                    def default_serializer(obj):
                                        if hasattr(obj, 'to_struct'):
                                            return obj.to_struct()
                                        else:
                                            raise TypeError("Object of type '%s' is not JSON serializable and does not have .to_struct() method." % obj.__class__.__name__)
                                    return json.dumps(obj, default=default_serializer, sort_keys=True)

                                def _serialize_float(float_value: float) -> str:
                                    if isinstance(float_value, str):
                                        return float_value
                                    if not isinstance(float_value, (float, int)):
                                        raise TypeError('Value "{}" has type "{}" instead of float.'.format(str(float_value), str(type(float_value))))
                                    return str(float_value)

                                def _serialize_int(int_value: int) -> str:
                                    if isinstance(int_value, str):
                                        return int_value
                                    if not isinstance(int_value, int):
                                        raise TypeError('Value "{}" has type "{}" instead of int.'.format(str(int_value), str(type(int_value))))
                                    return str(int_value)

                                import json
                                import argparse
                                _parser = argparse.ArgumentParser(prog='Aggregate regression metrics from csv', description='Calculates regression metrics.\n\n    Annotations:\n        author: Alexey Volkov <alexey.volkov@ark-kun.com>')
                                _parser.add_argument("--metrics-1", dest="metrics_1", type=json.loads, required=True, default=argparse.SUPPRESS)
                                _parser.add_argument("--metrics-2", dest="metrics_2", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--metrics-3", dest="metrics_3", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--metrics-4", dest="metrics_4", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("--metrics-5", dest="metrics_5", type=json.loads, required=False, default=argparse.SUPPRESS)
                                _parser.add_argument("----output-paths", dest="_output_paths", type=str, nargs=6)
                                _parsed_args = vars(_parser.parse_args())
                                _output_files = _parsed_args.pop("_output_paths", [])

                                _outputs = aggregate_regression_metrics_from_csv(**_parsed_args)

                                _output_serializers = [
                                    _serialize_int,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_float,
                                    _serialize_json,

                                ]

                                import os
                                for idx, output_file in enumerate(_output_files):
                                    try:
                                        os.makedirs(os.path.dirname(output_file))
                                    except OSError:
                                        pass
                                    with open(output_file, 'w') as f:
                                        f.write(_output_serializers[idx](_outputs[idx]))
                            args:
                              - '--metrics-1'
                              - inputValue: metrics_1
                              - if:
                                  cond:
                                    isPresent: metrics_2
                                  then:
                                    - '--metrics-2'
                                    - inputValue: metrics_2
                              - if:
                                  cond:
                                    isPresent: metrics_3
                                  then:
                                    - '--metrics-3'
                                    - inputValue: metrics_3
                              - if:
                                  cond:
                                    isPresent: metrics_4
                                  then:
                                    - '--metrics-4'
                                    - inputValue: metrics_4
                              - if:
                                  cond:
                                    isPresent: metrics_5
                                  then:
                                    - '--metrics-5'
                                    - inputValue: metrics_5
                              - '----output-paths'
                              - outputPath: number_of_items
                              - outputPath: max_absolute_error
                              - outputPath: mean_absolute_error
                              - outputPath: mean_squared_error
                              - outputPath: root_mean_squared_error
                              - outputPath: metrics
                    arguments:
                      metrics_1:
                        taskOutput:
                          outputName: metrics
                          taskId: Calculate regression metrics from csv
                          type: JsonObject
                      metrics_2:
                        taskOutput:
                          outputName: metrics
                          taskId: Calculate regression metrics from csv 2
                          type: JsonObject
                      metrics_3:
                        taskOutput:
                          outputName: metrics
                          taskId: Calculate regression metrics from csv 3
                          type: JsonObject
                      metrics_4:
                        taskOutput:
                          outputName: metrics
                          taskId: Calculate regression metrics from csv 4
                          type: JsonObject
                      metrics_5:
                        taskOutput:
                          outputName: metrics
                          taskId: Calculate regression metrics from csv 5
                          type: JsonObject
                outputValues:
                  mean_absolute_error:
                    taskOutput:
                      outputName: mean_absolute_error
                      taskId: Aggregate regression metrics from csv
                      type: Float
                  mean_squared_error:
                    taskOutput:
                      outputName: mean_squared_error
                      taskId: Aggregate regression metrics from csv
                      type: Float
                  root_mean_squared_error:
                    taskOutput:
                      outputName: root_mean_squared_error
                      taskId: Aggregate regression metrics from csv
                      type: Float
                  metrics:
                    taskOutput:
                      outputName: metrics
                      taskId: Aggregate regression metrics from csv
                      type: JsonObject
          url: https://raw.githubusercontent.com/Ark-kun/pipeline_components/d8c4cf5e6403bc65bcf8d606e6baf87e2528a3dc/components/XGBoost/Cross_validation_for_regression/from_CSV/component.yaml
        annotations:
          editor.position: '{"x":60,"y":200,"width":180,"height":54}'
        arguments:
          data:
            taskOutput:
              taskId: dataset
              outputName: Table
    outputValues: {}
